{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aesthetics Score Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to train a Deep Learning model to predict the score given an image based on the Aesthetics of the image. We will use the Standard AVA dataset for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from os import path\n",
    "import json\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(scores, labels, N_class):\n",
    "    y = np.zeros((N_class, ))\n",
    "    for idx, score in enumerate(scores):\n",
    "        map_idx = int(labels[str(idx + 1)]) - 1\n",
    "        y[map_idx] += score\n",
    "    y = (y / sum(y)) if sum(y) else y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs_and_label(data_file, labels_file, n_imgs):\n",
    "    with open(labels_file, 'r') as fi:\n",
    "        labels = json.load(fi)\n",
    "    with open(data_file, 'r') as fi:\n",
    "        data = fi.readlines()\n",
    "\n",
    "    # Initialize matrices\n",
    "    data = sample(data, n_imgs)\n",
    "    count = 0\n",
    "    N_class = len(set(labels.values()))\n",
    "    total_imgs = len(data)\n",
    "    print(total_imgs)\n",
    "    X = np.empty((total_imgs, 224, 224, 3))\n",
    "    Y = np.empty((total_imgs, N_class))\n",
    "\n",
    "    for line in data:\n",
    "        values = line.rstrip().split()\n",
    "        scores = [int(v) for v in values[2: 12]]\n",
    "        y_score = get_score(scores, labels, N_class)\n",
    "\n",
    "        img_name = values[1] + '.jpg'\n",
    "        img_path = path.join(all_imgs_dir, img_name[:2], img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        try:\n",
    "            # change img scale\n",
    "            img = (img - 127.0) / 127.0\n",
    "\n",
    "            if img.shape[2] == 4:  # PNG image\n",
    "                img = img[:, :, :-1]\n",
    "            elif img.shape[2] == 1:  # Gray image\n",
    "                cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            img = img.reshape(1, img.shape[0], img.shape[1], 3)\n",
    "        except:\n",
    "            continue\n",
    "            print('************IMG reshaping issue******')\n",
    "\n",
    "        y_score = y_score.reshape(1, N_class)\n",
    "\n",
    "        X[count] = img\n",
    "        Y[count] = y_score\n",
    "        count += 1\n",
    "        if not (count % 100):\n",
    "            print('Processed ==>', count)\n",
    "    X = X[:count]\n",
    "    Y = Y[:count]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_imgs(dataset_dir):\n",
    "    dataset_name = path.split(dataset_dir)[1]\n",
    "    train_file = path.join(dataset_dir, dataset_name + '_train.txt')\n",
    "    val_file = path.join(dataset_dir, dataset_name + '_train.txt')\n",
    "    test_file = path.join(dataset_dir, dataset_name + '_test.txt')\n",
    "    labels_file = path.join(dataset_dir, dataset_name + '_labels.json')\n",
    "\n",
    "    train_x, train_y = load_imgs_and_label(train_file, labels_file, 1000)\n",
    "    val_x, val_y = load_imgs_and_label(val_file, labels_file, 200)\n",
    "    data = {'train': (train_x, train_y), 'val': (val_x, val_y)}\n",
    "    if path.exists(test_file):\n",
    "        test_x, test_y = load_imgs_and_label(test_file, labels_file, 200)\n",
    "        data['test'] = (test_x, test_y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Conv2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adam, Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic CPCPDHDH cnn architecture\n",
    "def model_CPCPDHDH(IMAGE_SIZE, N_class):\n",
    "    nb_filter_1 = 20\n",
    "    nb_filter_2 = 50\n",
    "    nb_conv_1 = 5\n",
    "    nb_conv_2 = 3\n",
    "\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution2D(nb_filter_1, nb_conv_1, activation='relu',\n",
    "                          input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "    cnn.add(MaxPooling2D(strides=(2, 2)))\n",
    "\n",
    "    cnn.add(Convolution2D(nb_filter_2, nb_conv_2, activation='relu'))\n",
    "    cnn.add(MaxPooling2D(strides=(2, 2)))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(500, activation='relu'))\n",
    "\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(500, activation='relu'))\n",
    "\n",
    "    cnn.add(Dense(N_class, activation=\"softmax\"))\n",
    "\n",
    "    adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    cnn.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adadelta',\n",
    "                metrics=['accuracy'])\n",
    "    cnn.summary()\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the training\n",
    "N_class = 2\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "image_size = 224\n",
    "\n",
    "# More advanced configurations\n",
    "learning_rate = 0.01\n",
    "optimizer = 'adadelta'\n",
    "val_split = 0.2\n",
    "\n",
    "# Data directories\n",
    "all_imgs_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/ava_img_groups'\n",
    "dataset_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/datasets/ava_5k_2cls'\n",
    "model_name = 'ava_5k_2cls.h5'\n",
    "model_path = path.join(dataset_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_data = load_data_from_imgs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = expt_data['train']\n",
    "X_val, Y_val = expt_data['val']\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_CPCPDHDH(image_size, N_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_loc)\n",
    "print('Training accuracy: ', history.history['acc'][-1])\n",
    "print('Validation accuracy: ', history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training','validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we verified the base case, now we will move on to the original problem of rating a photo from 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the training\n",
    "N_class = 2\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "image_size = 224\n",
    "\n",
    "# More advanced configurations\n",
    "learning_rate = 0.01\n",
    "optimizer = 'adadelta'\n",
    "val_split = 0.2\n",
    "\n",
    "# Data directories\n",
    "all_imgs_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/ava_img_groups'\n",
    "dataset_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/datasets/ava_10cls_10k'\n",
    "model_name = 'ava_10cls_10k.h5'\n",
    "model_path = path.join(dataset_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_data = load_data_from_imgs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = expt_data['train']\n",
    "X_val, Y_val = expt_data['val']\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_CPCPDHDH(image_size, N_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_loc)\n",
    "print('Training accuracy: ', history.history['acc'][-1])\n",
    "print('Validation accuracy: ', history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training','validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training data is not distributed uniformly among all the classes, for example, a particular class has more data than the other classes, the model tends to be biased towards that particular class. We call this a Class Imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_model = load_model(model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = expt_data['test']\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = loaded_model.predict(X_test, verbose=1)\n",
    "y_labels = np.argmax(Y_test)\n",
    "pred_labels = np.argmax(pred_arr)\n",
    "correct_preds = np.sum(y_labels == pred_labels)\n",
    "accuracy = correct_preds / len(y_labels) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the data distribution and understand how to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = np.argmax(Y_train)\n",
    "y_categ_count = np.unique(y_train_labels, return_counts=True)\n",
    "plt_labels, plt_height = list(zip(*sorted(zip(*np.unique(m, return_counts=True)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(plt_labels, plt_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straight forward idea to solve this problem is to balance the Training set. Choose the minimum possible count and choose those many images from all classes\n",
    "\n",
    "As we don't have enough images, we bucket them into 5 classes each with 1k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the training\n",
    "N_class = 5\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "image_size = 224\n",
    "\n",
    "# More advanced configurations\n",
    "learning_rate = 0.01\n",
    "optimizer = 'adadelta'\n",
    "val_split = 0.2\n",
    "\n",
    "# Data directories\n",
    "all_imgs_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/ava_img_groups'\n",
    "dataset_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/datasets/ava_1.5k_5cls'\n",
    "model_name = 'ava_1.5k_5cls.h5'\n",
    "model_path = path.join(dataset_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_data = load_data_from_imgs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = expt_data['train']\n",
    "X_val, Y_val = expt_data['val']\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_CPCPDHDH(image_size, N_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_loc)\n",
    "print('Training accuracy: ', history.history['acc'][-1])\n",
    "print('Validation accuracy: ', history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training','validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to analyze if our loss function makes sense\n",
    "\n",
    "Think if there is a better loss function that you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted L2 loss\n",
    "def weighted_l2(y_true, y_pred):\n",
    "    shape = 5\n",
    "    y_true_weighted = tf.multiply(y_true, tf.range(1.0, float(shape+1), 1.0))\n",
    "    max_idx = tf.cast(tf.round(tf.reduce_sum(y_true_weighted)), tf.int32) - 1\n",
    "\n",
    "    si = tf.cast(tf.stack([abs(itr - max_idx)+1 for itr in range(shape)]), tf.float32)\n",
    "    dif = y_true - y_pred  # vectors\n",
    "    sco = dif * si\n",
    "    return K.square(sco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define same model with the new loss function\n",
    "def model_CPCPDHDH(IMAGE_SIZE, N_class):\n",
    "    nb_filter_1 = 20\n",
    "    nb_filter_2 = 50\n",
    "    nb_conv_1 = 5\n",
    "    nb_conv_2 = 3\n",
    "\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution2D(nb_filter_1, nb_conv_1, activation='relu',\n",
    "                          input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "    cnn.add(MaxPooling2D(strides=(2, 2)))\n",
    "\n",
    "    cnn.add(Convolution2D(nb_filter_2, nb_conv_2, activation='relu'))\n",
    "    cnn.add(MaxPooling2D(strides=(2, 2)))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(500, activation='relu'))\n",
    "\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(500, activation='relu'))\n",
    "\n",
    "    cnn.add(Dense(N_class, activation=\"softmax\"))\n",
    "\n",
    "    adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    cnn.compile(loss=weighted_l2,\n",
    "                optimizer='adadelta',\n",
    "                metrics=['accuracy'])\n",
    "    cnn.summary()\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the training\n",
    "N_class = 5\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "image_size = 224\n",
    "\n",
    "# More advanced configurations\n",
    "learning_rate = 0.01\n",
    "optimizer = 'adadelta'\n",
    "val_split = 0.2\n",
    "\n",
    "# Data directories\n",
    "all_imgs_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/ava_img_groups'\n",
    "dataset_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/datasets/ava_1.5k_5cls'\n",
    "model_name = 'ava_1.5k_5cls.h5'\n",
    "model_path = path.join(dataset_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_data = load_data_from_imgs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = expt_data['train']\n",
    "X_val, Y_val = expt_data['val']\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_CPCPDHDH(image_size, N_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_loc)\n",
    "print('Training accuracy: ', history.history['acc'][-1])\n",
    "print('Validation accuracy: ', history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training','validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG Architecture\n",
    "def model_vgg16(IMAGE_SIZE, N_class):\n",
    "    '''\n",
    "        Build/Define  VGG16 and load pretrained weights. used for all experiments\n",
    "        with regularisation, dropout etc...\n",
    "    '''\n",
    "    input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "  \n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    M = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(M)\n",
    "    model = Model(img_input, x)\n",
    "    F = Flatten(name='flatten')(M)\n",
    "    FC1 = Dense(4096, activation='relu', name='fc1')(F)\n",
    "    D1 = Dropout(0.5)(FC1)\n",
    "    FC2 = Dense(500, activation='relu', name='fc2')(D1)\n",
    "    D2 = Dropout(0.5)(FC2)\n",
    "    y = Dense(N_class, activation='softmax', name='predictions')(x)\n",
    "    model = Model(img_input, y)\n",
    "\n",
    "    # Compile model with above settings\n",
    "    model.compile(loss=weighted_l2, optimizer='adadelta', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the training\n",
    "N_class = 5\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "image_size = 224\n",
    "\n",
    "# More advanced configurations\n",
    "learning_rate = 0.01\n",
    "optimizer = 'adadelta'\n",
    "val_split = 0.2\n",
    "\n",
    "# Data directories\n",
    "all_imgs_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/ava_img_groups'\n",
    "dataset_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/datasets/ava_1.5k_5cls'\n",
    "model_name = 'ava_1.5k_5cls.h5'\n",
    "model_path = path.join(dataset_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_data = load_data_from_imgs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = expt_data['train']\n",
    "X_val, Y_val = expt_data['val']\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_CPCPDHDH(image_size, N_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_loc)\n",
    "print('Training accuracy: ', history.history['acc'][-1])\n",
    "print('Validation accuracy: ', history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training','validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, more than measuring the accuracy as usual,\n",
    "\n",
    "1. Actual 1 ==> Predicted 1 - Correct\n",
    "2. Actual 2 ==> Predicted 5 - Wrong\n",
    "3. Actual 2 ==> Predicted 3 - Wrong\n",
    "\n",
    "I the above case, I am okay with the case 3, where 2 got predicted as 3. But case 2 is bad where 2 was predicted as 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_score(scores):\n",
    "    si = np.arange(1, len(scores)+1, 1)\n",
    "    mean = np.sum(scores * si)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = expt_data['test']\n",
    "pred_arr = model.predict(X_test, verbose=1)\n",
    "\n",
    "off_by_count = {'0.5': 0, '1.0': 0, '1.5': 0, '2.0': 0}\n",
    "\n",
    "for act, pred in zip(Y_test, pred_arr):\n",
    "    mean_act = mean_score(act)\n",
    "    mean_pred = mean_score(pred)\n",
    "    \n",
    "    # Generate accuracy report with Off by different values\n",
    "    # The values typically are 0.5, 1, 1.5, 2 etc\n",
    "    diff = abs(act_aes - pred_aes)\n",
    "    for thresh in off_by_count:\n",
    "        if diff <= float(thresh):\n",
    "            off_by_count[thresh] += 1\n",
    "print(off_by_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
