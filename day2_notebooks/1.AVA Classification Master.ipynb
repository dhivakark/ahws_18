{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aesthetics Score Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to train a Deep Learning model to predict the score given an image based on the Aesthetics of the image. We will use the Standard AVA dataset for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from os import path\n",
    "import json\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(scores, labels, N_class):\n",
    "    y = np.zeros((N_class, ))\n",
    "    for idx, score in enumerate(scores):\n",
    "        map_idx = int(labels[str(idx + 1)]) - 1\n",
    "        y[map_idx] += score\n",
    "    y = (y / sum(y)) if sum(y) else y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs_and_label(data_file, labels_file, n_imgs):\n",
    "    with open(labels_file, 'r') as fi:\n",
    "        labels = json.load(fi)\n",
    "    with open(data_file, 'r') as fi:\n",
    "        data = fi.readlines()\n",
    "\n",
    "    # Initialize matrices\n",
    "    data = sample(data, n_imgs)\n",
    "    count = 0\n",
    "    N_class = len(set(labels.values()))\n",
    "    total_imgs = len(data)\n",
    "    print(total_imgs)\n",
    "    X = np.empty((total_imgs, 224, 224, 3))\n",
    "    Y = np.empty((total_imgs, N_class))\n",
    "\n",
    "    for line in data:\n",
    "        values = line.rstrip().split()\n",
    "        scores = [int(v) for v in values[2: 12]]\n",
    "        y_score = get_score(scores, labels, N_class)\n",
    "\n",
    "        img_name = values[1] + '.jpg'\n",
    "        img_path = path.join(all_imgs_dir, img_name[:2], img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        try:\n",
    "            # change img scale\n",
    "            img = (img - 127.0) / 127.0\n",
    "\n",
    "            if img.shape[2] == 4:  # PNG image\n",
    "                img = img[:, :, :-1]\n",
    "            elif img.shape[2] == 1:  # Gray image\n",
    "                cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "            img = img.reshape(1, img.shape[0], img.shape[1], 3)\n",
    "        except:\n",
    "            continue\n",
    "            print('************IMG reshaping issue******')\n",
    "\n",
    "        y_score = y_score.reshape(1, N_class)\n",
    "\n",
    "        X[count] = img\n",
    "        Y[count] = y_score\n",
    "        count += 1\n",
    "        if not (count % 100):\n",
    "            print('Processed ==>', count)\n",
    "    X = X[:count]\n",
    "    Y = Y[:count]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_imgs(dataset_dir):\n",
    "    dataset_name = path.split(dataset_dir)[1]\n",
    "    train_file = path.join(dataset_dir, dataset_name + '_train.txt')\n",
    "    val_file = path.join(dataset_dir, dataset_name + '_train.txt')\n",
    "    test_file = path.join(dataset_dir, dataset_name + '_test.txt')\n",
    "    labels_file = path.join(dataset_dir, dataset_name + '_labels.json')\n",
    "\n",
    "    train_x, train_y = load_imgs_and_label(train_file, labels_file, 1000)\n",
    "    val_x, val_y = load_imgs_and_label(val_file, labels_file, 200)\n",
    "    data = {'train': (train_x, train_y), 'val': (val_x, val_y)}\n",
    "    if path.exists(test_file):\n",
    "        test_x, test_y = load_imgs_and_label(test_file, labels_file, 200)\n",
    "        data['test'] = (test_x, test_y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Conv2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adam, Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic CPCPDHDH cnn architecture\n",
    "def model_CPCPDHDH(IMAGE_SIZE, N_class):\n",
    "    nb_filter_1 = 20\n",
    "    nb_filter_2 = 50\n",
    "    nb_conv_1 = 5\n",
    "    nb_conv_2 = 3\n",
    "\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution2D(nb_filter_1, nb_conv_1, activation='relu',\n",
    "                          input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "    cnn.add(MaxPooling2D(strides=(2, 2)))\n",
    "\n",
    "    cnn.add(Convolution2D(nb_filter_2, nb_conv_2, activation='relu'))\n",
    "    cnn.add(MaxPooling2D(strides=(2, 2)))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(500, activation='relu'))\n",
    "\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(500, activation='relu'))\n",
    "\n",
    "    cnn.add(Dense(N_class, activation=\"softmax\"))\n",
    "\n",
    "    adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    cnn.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adadelta',\n",
    "                metrics=['accuracy'])\n",
    "    cnn.summary()\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the training\n",
    "N_class = 2\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "image_size = 224\n",
    "\n",
    "# More advanced configurations\n",
    "learning_rate = 0.01\n",
    "optimizer = 'adadelta'\n",
    "val_split = 0.2\n",
    "\n",
    "# Data directories\n",
    "all_imgs_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/ava_img_groups'\n",
    "dataset_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/datasets/ava_5k_2cls'\n",
    "model_name = 'ava_5k_2cls.h5'\n",
    "model_path = path.join(dataset_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_data = load_data_from_imgs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = expt_data['train']\n",
    "X_val, Y_val = expt_data['val']\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_CPCPDHDH(image_size, N_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_loc)\n",
    "print('Training accuracy: ', history.history['acc'][-1])\n",
    "print('Validation accuracy: ', history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training','validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we verified the base case, now we will move on to the original problem of rating a photo from 1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the training\n",
    "N_class = 2\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "image_size = 224\n",
    "\n",
    "# More advanced configurations\n",
    "learning_rate = 0.01\n",
    "optimizer = 'adadelta'\n",
    "val_split = 0.2\n",
    "\n",
    "# Data directories\n",
    "all_imgs_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/ava_img_groups'\n",
    "dataset_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/datasets/ava_10cls_10k'\n",
    "model_name = 'ava_10cls_10k.h5'\n",
    "model_path = path.join(dataset_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_data = load_data_from_imgs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = expt_data['train']\n",
    "X_val, Y_val = expt_data['val']\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_CPCPDHDH(image_size, N_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_loc)\n",
    "print('Training accuracy: ', history.history['acc'][-1])\n",
    "print('Validation accuracy: ', history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training','validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training data is not distributed uniformly among all the classes, for example, a particular class has more data than the other classes, the model tends to be biased towards that particular class. We call this a Class Imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_model = load_model(model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = expt_data['test']\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = loaded_model.predict(X_test, verbose=1)\n",
    "y_labels = np.argmax(Y_test)\n",
    "pred_labels = np.argmax(pred_arr)\n",
    "correct_preds = np.sum(y_labels == pred_labels)\n",
    "accuracy = correct_preds / len(y_labels) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the data distribution and understand how to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = np.argmax(Y_train)\n",
    "y_categ_count = np.unique(y_train_labels, return_counts=True)\n",
    "plt_labels, plt_height = list(zip(*sorted(zip(*np.unique(m, return_counts=True)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(plt_labels, plt_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straight forward idea to solve this problem is to balance the Training set. Choose the minimum possible count and choose those many images from all classes\n",
    "\n",
    "As we don't have enough images, we bucket them into 5 classes each with 1k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the training\n",
    "N_class = 5\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "image_size = 224\n",
    "\n",
    "# More advanced configurations\n",
    "learning_rate = 0.01\n",
    "optimizer = 'adadelta'\n",
    "val_split = 0.2\n",
    "\n",
    "# Data directories\n",
    "all_imgs_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/ava_img_groups'\n",
    "dataset_dir = '/home/dhivakar/work/projects/scancafe/sl_git/sc_as/data/ava/subsets/ahws_18/datasets/ava_1.5k_5cls'\n",
    "model_name = 'ava_1.5k_5cls.h5'\n",
    "model_path = path.join(dataset_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_data = load_data_from_imgs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = expt_data['train']\n",
    "X_val, Y_val = expt_data['val']\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_CPCPDHDH(image_size, N_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_loc)\n",
    "print('Training accuracy: ', history.history['acc'][-1])\n",
    "print('Validation accuracy: ', history.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training','validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
