{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to analyze if our loss function makes sense\n",
    "\n",
    "Think if there is a better loss function that you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement L2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted L2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X and Y from h5 files\n",
    "def read_data_from_h5_files(x_path, y_path, labels_file):\n",
    "    '''\n",
    "        Read saved h5 files, shuffle, write the dataset split image ids in files\n",
    "    '''\n",
    "    # Load data H5 files\n",
    "    hf1 = h5py.File(x_path, 'r+')\n",
    "    X_train = hf1['X_train'].value\n",
    "    hf1.close()\n",
    "    hf2 = h5py.File(y_path, 'r+')\n",
    "    Y_train = hf2['Y_train'].value\n",
    "    hf2.close()\n",
    "    print('Data loaded from h5 files')\n",
    "\n",
    "    idxs = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(idxs)\n",
    "    X_train = X_train[idxs]\n",
    "    Y_train = Y_train[idxs]\n",
    "    print('Data shuffled')\n",
    "\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic CPCPDHDH cnn architecture\n",
    "def model_CPCPDHDH(IMAGE_SIZE, WEIGHT_FILE, N_class):\n",
    "    nb_filter_1 = 20\n",
    "    nb_filter_2 = 50\n",
    "    nb_conv_1 = 5\n",
    "    nb_conv_2 = 3\n",
    "\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution2D(nb_filter_1, nb_conv_1, activation='relu',\n",
    "                          input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "    cnn.add(MaxPooling2D(strides=(2, 2)))\n",
    "\n",
    "    cnn.add(Convolution2D(nb_filter_2, nb_conv_2, activation='relu'))\n",
    "    cnn.add(MaxPooling2D(strides=(2, 2)))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(500, activation='relu'))\n",
    "\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(500, activation='relu'))\n",
    "\n",
    "    cnn.add(Dense(N_class, activation=\"softmax\"))\n",
    "\n",
    "    adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    cnn.compile(loss='categorical_cross_entropy',\n",
    "                optimizer='adadelta',\n",
    "                metrics=['accuracy'])\n",
    "    cnn.summary()\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for the training\n",
    "n_class = 2\n",
    "batch_size = 50\n",
    "epochs = 30\n",
    "image_size = 224\n",
    "\n",
    "# More advanced configurations\n",
    "learning_rate = 0.01\n",
    "optimizer = 'adadelta'\n",
    "validation_split = 0.2\n",
    "\n",
    "# Data directories\n",
    "x_train_path = ''\n",
    "y_train_path = ''\n",
    "#labels_file = path.join(ds_base_dir, 'subsets/'+CONFIG['dataset_name']+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_data_from_h5_files(x_train_path, y_train_path, labels_file)\n",
    "model = model_CPCPDHDH(image_size, N_class)\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_loc)\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "train_acc = history.history['acc']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
